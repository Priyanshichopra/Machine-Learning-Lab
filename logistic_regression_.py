# -*- coding: utf-8 -*-
"""Logistic Regression .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MvEOuMe-bAvm1jClq1Zl3VvuR9Nf-pxf

###Logistic Regression (1)

Import modules
"""

import numpy as np
import pandas as pd

"""Normalization : Min-Max Scaler"""

def normalize(X):
  mins = np.min(X, axis=0)
  maxs = np.max(X,axis=0)

  diff = maxs-mins
  print(mins,maxs)
  return 1-((maxs-X)/diff)

"""Pre-Activation and Activation Calculation"""

def sigmoid(X,theta):
  return 1.0/(1 + np.exp(-np.dot(X, theta.T)))

"""Cost Function"""

def cf(X,Y,theta):
  # COST = Y*log(h(X)) - (1-Y)*log(1-h(X))
  cost = Y*np.log(sigmoid(X,theta)) - (1-Y)*np.log(1-sigmoid(X,theta))
  return np.mean(cost)

"""Calculating Derivative"""

def derivative(X, Y, theta):
  # DERIVATIVE = [Y(PREDICTION) - Y(ACTUAL)].TRANSPOSE * X
  return np.dot((sigmoid(X,theta) - Y.reshape(X.shape[0], -1)).T,X)

"""Gradient Descent Algorihm"""

def gradient_descent(X,Y,theta,lr=0.01,conv=.001):
  cost = cf(X,Y,theta)
  change = 1
  epochs = 1
  while(change>conv):
    old_cost = cost
    theta = theta - lr*derivative(X,Y,theta)
    cost = cf(X,Y,theta)
    change = old_cost-cost
    epochs+=1
  return theta, epochs

"""Predict"""

def predict(X,theta):
  Y_pred = sigmoid(X,theta)
  Y_pred = np.where(Y_pred>0.5, 1, 0)
  return np.squeeze(Y_pred)

"""Main Method"""

if __name__ == "__main__":
  data = np.array(pd.read_csv("dataset1.csv"))

  X = data[:,:2]
  Y = data[:,2]
  #Normalizing the dataset : converting it's value between 0 and 1
  X = normalize(X)
  #horizontal stacking of ones in the X for bias
  X = np.hstack((np.matrix(np.ones(X.shape[0])).T,X))
  
  theta = np.matrix(np.zeros(X.shape[1]))

  theta, epochs = gradient_descent(X, Y, theta)

  Y_predicted = predict(X,theta)
  print(theta, epochs)
  print("Correctly predicted: ", np.sum(Y==Y_predicted))

"""###Logistic Regression using Z-score Scaler"""

import csv 
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
  
def MinMax(X): 
    ''' 
    function to normalize feature matrix, X 
    '''
    mins = np.min(X, axis = 0) 
    maxs = np.max(X, axis = 0) 
    rng = maxs - mins 
    norm_X = 1 - ((maxs - X)/rng) 
    return norm_X 
  
  
def ZScore(X):
    '''
    function to normalize feature matrix, X
    '''
    diff = X - np.mean(X)
    norm_X = diff/np.std(X)
    return norm_X


def logistic_func(theta, X): 
    ''' 
    logistic(sigmoid) function 1/1+e^-theta.T * X
    '''
    return 1.0/(1 + np.exp(-np.dot(X, theta.T))) 
  
  
def log_gradient(theta, X, y): 
    ''' 
    logistic gradient function [Y(PREDICTION)- Y(ACTUAL)).Transpose *  X
    '''
    first_calc = logistic_func(theta, X) - y.reshape(X.shape[0], -1) 
    final_calc = np.dot(first_calc.T, X) 
    return final_calc 
  
  
def cost_func(theta, X, y): 
    ''' 
    cost function, J 
    '''
    log_func_v = logistic_func(theta, X) 
    y = np.squeeze(y) 
    step1 = y * np.log(log_func_v) 
    step2 = (1 - y) * np.log(1 - log_func_v) 
    final = -step1 - step2 
    return np.mean(final) 
  
  
def grad_desc(X, y, theta, lr=.01, converge_change=.001): 
    ''' 
    gradient descent function 
    '''
    cost = cost_func(theta, X, y) 
    change_cost = 1
    num_iter = 1
      
    while(change_cost > converge_change): 
        old_cost = cost 
        theta = theta - (lr * log_gradient(theta, X, y)) 
        cost = cost_func(theta, X, y) 
        change_cost = old_cost - cost 
        num_iter += 1
      
    return theta, num_iter  
  
  
def pred_values(theta, X): 
    ''' 
    function to predict labels 
    '''
    pred_prob = logistic_func(theta, X) 
    pred_value = np.where(pred_prob >= .5, 1, 0) 
    return np.squeeze(pred_value) 
  
  
def plot_reg(X, y, theta): 
    ''' 
    function to plot decision boundary 
    '''
    # labelled observations 
    x_0 = X[np.where(y == 0.0)] 
    x_1 = X[np.where(y == 1.0)] 
      
    # plotting points with diff color for diff label 
    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') 
    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') 
      
    # plotting decision boundary 
    x1 = np.arange(0, 1, 0.1) 
    x2 = -(theta[0,0] + theta[0,1]*x1)/theta[0,2] 
    plt.plot(x1, x2, c='k', label='reg line') 
  
    plt.xlabel('x1') 
    plt.ylabel('x2') 
    plt.legend() 
    plt.show() 
      
  
      
if __name__ == "__main__": 
    # load the dataset

    dataset = pd.read_csv('dataset1.csv')  
    dataset=np.array(dataset)
    # normalizing feature matrix 
    X = ZScore(dataset[:, :-1]) 
    # stacking columns wth all ones in feature matrix 
    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X)) 
    print(X)
    # response vector 
    y = dataset[:, -1] 
    print (y )
    # initial beta values 
    theta = np.matrix(np.zeros(X.shape[1])) 
    print (theta )
    # beta values after running gradient descent 
    theta, num_iter = grad_desc(X, y, theta) 
  
    # estimated beta values and number of iterations 
    print("Estimated regression coefficients:", theta) 
    print("No. of iterations:", num_iter) 
  
    # predicted labels 
    y_pred = pred_values(theta, X) 
      
    # number of correctly predicted labels 
    print("Correctly predicted labels:", np.sum(y == y_pred)) 
      
    # plotting regression line 
    plot_reg(X, y, theta)

"""###Logistic Regression using Min-Max Scaler"""

import csv 
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
  
def MinMax(X): 
    ''' 
    function to normalize feature matrix, X 
    '''
    mins = np.min(X, axis = 0) 
    maxs = np.max(X, axis = 0) 
    rng = maxs - mins 
    norm_X = 1 - ((maxs - X)/rng) 
    return norm_X 
  
  
def ZScore(X):
    '''
    function to normalize feature matrix, X
    '''
    diff = X - np.mean(X)
    norm_X = diff/np.std(X)
    return norm_X


def logistic_func(theta, X): 
    ''' 
    logistic(sigmoid) function 1/1+e^-theta.T * X
    '''
    return 1.0/(1 + np.exp(-np.dot(X, theta.T))) 
  
  
def log_gradient(theta, X, y): 
    ''' 
    logistic gradient function [Y(PREDICTION)- Y(ACTUAL)).Transpose *  X
    '''
    first_calc = logistic_func(theta, X) - y.reshape(X.shape[0], -1) 
    final_calc = np.dot(first_calc.T, X) 
    return final_calc 
  
  
def cost_func(theta, X, y): 
    ''' 
    cost function, J 
    '''
    log_func_v = logistic_func(theta, X) 
    y = np.squeeze(y) 
    step1 = y * np.log(log_func_v) 
    step2 = (1 - y) * np.log(1 - log_func_v) 
    final = -step1 - step2 
    return np.mean(final) 
  
  
def grad_desc(X, y, theta, lr=.01, converge_change=.001): 
    ''' 
    gradient descent function 
    '''
    cost = cost_func(theta, X, y) 
    change_cost = 1
    num_iter = 1
      
    while(change_cost > converge_change): 
        old_cost = cost 
        theta = theta - (lr * log_gradient(theta, X, y)) 
        cost = cost_func(theta, X, y) 
        change_cost = old_cost - cost 
        num_iter += 1
      
    return theta, num_iter  
  
  
def pred_values(theta, X): 
    ''' 
    function to predict labels 
    '''
    pred_prob = logistic_func(theta, X) 
    pred_value = np.where(pred_prob >= .5, 1, 0) 
    return np.squeeze(pred_value) 
  
  
def plot_reg(X, y, theta): 
    ''' 
    function to plot decision boundary 
    '''
    # labelled observations 
    x_0 = X[np.where(y == 0.0)] 
    x_1 = X[np.where(y == 1.0)] 
      
    # plotting points with diff color for diff label 
    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') 
    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') 
      
    # plotting decision boundary 
    x1 = np.arange(0, 1, 0.1) 
    x2 = -(theta[0,0] + theta[0,1]*x1)/theta[0,2] 
    plt.plot(x1, x2, c='k', label='reg line') 
  
    plt.xlabel('x1') 
    plt.ylabel('x2') 
    plt.legend() 
    plt.show() 
      
  
      
if __name__ == "__main__": 
    # load the dataset

    dataset = pd.read_csv('dataset1.csv')  
    dataset=np.array(dataset)
    # normalizing feature matrix 
    X = MinMax(dataset[:, :-1]) 
    # stacking columns wth all ones in feature matrix 
    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X)) 
    print(X)
    # response vector 
    y = dataset[:, -1] 
    print (y )
    # initial beta values 
    theta = np.matrix(np.zeros(X.shape[1])) 
    print (theta )
    # beta values after running gradient descent 
    theta, num_iter = grad_desc(X, y, theta) 
  
    # estimated beta values and number of iterations 
    print("Estimated regression coefficients:", theta) 
    print("No. of iterations:", num_iter) 
  
    # predicted labels 
    y_pred = pred_values(theta, X) 
      
    # number of correctly predicted labels 
    print("Correctly predicted labels:", np.sum(y == y_pred)) 
      
    # plotting regression line 
    plot_reg(X, y, theta)